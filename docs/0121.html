<html>
<head>
<title>Copying TB’s of data between s3 buckets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 s3 存储桶之间复制 TB 数据</h1>
<blockquote>原文：<a href="https://medium.com/javarevisited/copying-tbs-of-data-between-s3-buckets-8438dde7dadb?source=collection_archive---------0-----------------------#2019-09-12">https://medium.com/javarevisited/copying-tbs-of-data-between-s3-buckets-8438dde7dadb?source=collection_archive---------0-----------------------#2019-09-12</a></blockquote><div><div class="dt gx gy gz ha hb"/><div class="hc hd he hf hg"><figure class="hi hj fa fc hk hl es et paragraph-image"><a href="https://medium.com/@javinpaul/top-5-aws-training-courses-to-crack-amazon-web-service-solutions-architect-associate-certification-3f4affa8f660"><div class="es et hh"><img src="../Images/ed0b74e75de3f0c2148497e0e7334a93.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*Bh_ceYx90Y8zHKROAbpq5A.jpeg"/></div></a></figure><div class=""/></div><div class="ab cl in io gq ip" role="separator"><span class="iq bw bk ir is it"/><span class="iq bw bk ir is it"/><span class="iq bw bk ir is"/></div><div class="hc hd he hf hg"><h1 id="edd9" class="iu iv hq bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">问题陈述:</h1><p id="f7b9" class="pw-post-body-paragraph js jt hq ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hc bi translated">作为常规生产升级的一部分，我们尝试在 s3 存储桶中备份数据</p><p id="f64f" class="pw-post-body-paragraph js jt hq ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hc bi translated">项目数:1，000，344，大小:~130 GB</p><p id="651c" class="pw-post-body-paragraph js jt hq ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hc bi translated">我们基本上是使用常规的 s3 命令启动备份，例如:</p><pre class="kv kw kx ky fe kz la lb lc aw ld bi"><span id="d626" class="le iv hq la b fj lf lg l lh li">aws s3 cp --recursive s3://&lt;bucket&gt;&gt;<br/>aws s3 sync s3://&lt;bucket&gt; s3://&lt;bucket&gt;&gt;</span></pre><p id="5ffb" class="pw-post-body-paragraph js jt hq ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hc bi translated">在执行过程中，我们注意到执行复制花费了数小时，没有办法让它更快，我们找到的唯一解决方法是在多个终端中并行运行这些<a class="ae lj" rel="noopener" href="/javarevisited/top-5-aws-training-courses-to-crack-amazon-web-service-solutions-architect-associate-certification-3f4affa8f660?source=collection_home---4------0-----------------------"> aws 命令</a>，以便它们可以同时在不同的 s3 分区上操作，并更快地执行复制，这既不是一个优雅的解决方案，也不可扩展。</p><h1 id="7e03" class="iu iv hq bd iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr bi translated">其他选项:</h1><p id="2749" class="pw-post-body-paragraph js jt hq ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hc bi translated">我们尝试了堆栈溢出和 AWS 论坛中提到的其他选项，如</p><h2 id="dc6e" class="le iv hq bd iw lp lq lr ja ls lt lu je kd lv lw ji kh lx ly jm kl lz ma jq mb bi translated">S3 批处理操作</h2><p id="5e34" class="pw-post-body-paragraph js jt hq ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hc bi translated">S3 批处理操作似乎解决了这个问题，但目前它还不支持基于 KMS 密钥加密的对象。当我创建一个作业来拷贝启用了 KMS 密钥加密的存储桶的内容时，出现了以下错误:</p><p id="ddea" class="pw-post-body-paragraph js jt hq ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hc bi translated">使用了不支持的加密类型:SSE_KMS</p><p id="3e83" class="pw-post-body-paragraph js jt hq ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hc bi translated">当我进一步了解 AWS 文档时，它在“指定清单”一节中指出，不支持使用带有客户提供的密钥的服务器端加密(SSE-C)和带有 AWS KMS 托管密钥的服务器端加密(SSE-KMS)的清单</p><p id="be48" class="pw-post-body-paragraph js jt hq ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hc bi translated"><a class="ae lj" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops-basics.html#specify-batchjob-manifest" rel="noopener ugc nofollow" target="_blank">https://docs . AWS . Amazon . com/Amazon S3/latest/dev/batch-ops-basics . html # specify-batch job-manifest</a></p><h2 id="afbe" class="le iv hq bd iw lp lq lr ja ls lt lu je kd lv lw ji kh lx ly jm kl lz ma jq mb bi translated">s3-dist-cp</h2><p id="3bc0" class="pw-post-body-paragraph js jt hq ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hc bi translated">s3-dist-cp 似乎很有希望，但是当我对一个具有接近 6 TB 数据的存储桶运行它时，该作业在运行“reduce”任务 40 分钟后失败，没有任何失败的明确指示</p><h1 id="8ca6" class="iu iv hq bd iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr bi translated">定制方法:</h1><p id="c73f" class="pw-post-body-paragraph js jt hq ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hc bi translated">不幸的是，上面提到的方法都没有解决我们的问题，所以我们想出了这个方法。这种方法可以进一步优化，所以把它作为解决这个问题的第一步。</p><p id="61c7" class="pw-post-body-paragraph js jt hq ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hc bi translated">这是一个两步的过程，它是 shell 脚本和 spark 代码的结合。首先我们需要生成记录文件(带有对象键)，然后运行 spark 代码在多个任务中跨节点并行复制文件</p><h2 id="4bd8" class="le iv hq bd iw lp lq lr ja ls lt lu je kd lv lw ji kh lx ly jm kl lz ma jq mb bi translated">生成记录文件:</h2><p id="f39a" class="pw-post-body-paragraph js jt hq ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hc bi translated">我们需要生成一个文本文件，包含源 s3 存储桶(将被复制)中的项目的对象键，这可以通过在任何<a class="ae lj" rel="noopener" href="/javarevisited/top-5-aws-training-courses-to-crack-amazon-web-service-solutions-architect-associate-certification-3f4affa8f660?source=collection_home---4------0-----------------------"> EC2 实例</a>上运行这个命令来完成:</p><pre class="kv kw kx ky fe kz la lb lc aw ld bi"><span id="40e8" class="le iv hq la b fj lf lg l lh li">aws s3 ls s3://test_bucket --recursive | awk '{print $4}' &gt; /tmp/output.txt</span></pre><p id="c270" class="pw-post-body-paragraph js jt hq ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hc bi translated"><strong class="ju hr">输出:</strong>(仅每行一个对象键)</p><p id="e11d" class="pw-post-body-paragraph js jt hq ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hc bi translated">data/solution = 33/test1 . mov<br/>data/solution = 33/test2 . mov 等</p><h2 id="82b8" class="le iv hq bd iw lp lq lr ja ls lt lu je kd lv lw ji kh lx ly jm kl lz ma jq mb bi translated"><a class="ae lj" rel="noopener" href="/javarevisited/5-free-courses-to-learn-apache-spark-in-2020-bdff2d60c800">火花码</a>:</h2><pre class="kv kw kx ky fe kz la lb lc aw ld bi"><span id="721d" class="le iv hq la b fj lf lg l lh li">sql.read()<br/> .textFile(file)<br/> .repartition(2000)<br/> .flatMap((FlatMapFunction&lt;String, String&gt;) s -&gt; Arrays.asList(s.split("\n")).iterator(), Encoders.STRING())<br/> .map((MapFunction&lt;String, String&gt;) s -&gt; String.format("aws s3 cp %s s3://%s/%s", String.format("s3://%s/%s", source, s), target, s), Encoders.STRING())<br/> .foreachPartition((ForeachPartitionFunction&lt;String&gt;) iterator -&gt; {<br/>       while (iterator.hasNext())<br/>         Runtime.getRuntime().exec(iterator.next()).waitFor();<br/> });</span></pre><p id="63c2" class="pw-post-body-paragraph js jt hq ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hc bi translated"><strong class="ju hr">星火提交:</strong></p><p id="8c72" class="pw-post-body-paragraph js jt hq ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hc bi translated">spark-submit-class com . S3 . S3 copy S3://test _ bucket/copier . jar test _ bucket back _ up _ bucket S3://test _ bucket/output . txt</p><p id="d3f7" class="pw-post-body-paragraph js jt hq ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hc bi translated">args[0] →源桶</p><p id="3af2" class="pw-post-body-paragraph js jt hq ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hc bi translated">args[1] →目标存储桶</p><p id="905f" class="pw-post-body-paragraph js jt hq ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hc bi translated">args[3] →上一步中生成的 s3 记录文件</p><p id="58b0" class="pw-post-body-paragraph js jt hq ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hc bi translated">这段代码将读取“output.txt”文件，并将其分成多个分区，然后跨多个节点并行运行它们。</p><h1 id="cb31" class="iu iv hq bd iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr bi translated">特性试验</h1><p id="49f6" class="pw-post-body-paragraph js jt hq ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hc bi translated">使用 15 个 EMR 核心节点，每个 m4.xlarge 实例类型，我们能够在不到<strong class="ju hr"> 40 分钟</strong>的时间内复制<strong class="ju hr"> 5.5 TB </strong>的数据。因为我们只为使用 EMR 的时间付费，所以它具有成本效益(通过 SPOT 或 EC2 车队配置可以进一步降低成本),并且与之前的方法相比具有更大的可扩展性。</p><h1 id="be72" class="iu iv hq bd iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn lo jp jq jr bi translated">Spark 提交:</h1><p id="e03f" class="pw-post-body-paragraph js jt hq ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hc bi translated">spark-submit—conf spark . network . time out = 420000s—conf spark . executor . heartbeat interval = 410000s—conf spark . yarn . scheduler . mode = FAIR—conf spark . shuffle . service . enabled = true—conf spark . serializer = org . Apache . spark . serializer . kryoserializer—conf spark . executor . memoryoverhead = 1024—conf spark . driver . memoryoverhead = 1024—conf spark . executor . instances = 74—conf spark .</p></div></div>    
</body>
</html>