<html>
<head>
<title>Apache Spark, Hive, and Spring Boot — Testing Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">阿帕奇火花，蜂巢和Spring Boot-测试指南</h1>
<blockquote>原文：<a href="https://medium.com/javarevisited/apache-spark-hive-and-spring-boot-testing-guide-42b07f2b480e?source=collection_archive---------0-----------------------#2022-04-23">https://medium.com/javarevisited/apache-spark-hive-and-spring-boot-testing-guide-42b07f2b480e?source=collection_archive---------0-----------------------#2022-04-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ba59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大数据是趋势。这些公司必须处理大量的数据才能与其他人竞争。例如，这些信息用于向您显示相关广告，并向您推荐您可能感兴趣的服务。</p><p id="ea38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大数据软件系统的问题在于其复杂性。测试变得艰难。当应用程序被调整为连接到HDFS集群时，如何在本地验证它的行为？</p><p id="e59a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我将向您展示如何创建一个Spring Boot应用程序，该应用程序通过<a class="ae jd" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>将数据从<a class="ae jd" href="https://hive.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Hive </a>加载到<a class="ae jd" href="https://aerospike.com/" rel="noopener ugc nofollow" target="_blank"> Aerospike数据库</a>。不仅如此，我还将为您提供为这种场景编写集成测试的方法，这些测试可以在本地运行，也可以在CI管道执行期间运行。代码示例取自<a class="ae jd" href="https://github.com/SimonHarmonicMinor/apache-spark-integration-testing-example" rel="noopener ugc nofollow" target="_blank">这个库</a>。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><a href="https://javarevisited.blogspot.com/2018/04/top-5-hadoop-courses-to-learn-online.html"><div class="er es je"><img src="../Images/4a9cebd26b6f14784ff53ef8085aad75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/0*ImJ6wKOjiv57E_1w.jpeg"/></div></a></figure><p id="e818" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，让我们了解一下我们正在使用的大数据堆栈的一些基本概念。别担心，不会太久的。但理解核心思想是必要的。</p><h1 id="31c7" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">HDFS基础知识</h1><p id="51fe" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated"><a class="ae jd" href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html" rel="noopener ugc nofollow" target="_blank"> HDFS (Hadoop分布式文件系统)</a>是一个分布式文件系统，设计用于在许多物理服务器上运行。因此，HDFS中的文件是一种抽象，它隐藏了在多个节点之间存储和复制数据的复杂性。我们为什么需要HDFS？有一些原因。</p><h2 id="8d97" class="kp jn hi bd jo kq kr ks js kt ku kv jw iq kw kx ka iu ky kz ke iy la lb ki lc bi translated">硬件故障</h2><p id="0f54" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">硬盘驱动器崩溃。这是我们必须面对的现实。如果一个文件在多个节点之间拆分，个别故障不会影响整个数据。此外，数据在HDFS复制。因此，即使在磁盘崩溃后，信息也可以从其他来源恢复。</p><h2 id="de65" class="kp jn hi bd jo kq kr ks js kt ku kv jw iq kw kx ka iu ky kz ke iy la lb ki lc bi translated">非常大的文件</h2><p id="539f" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">HDFS允许将一个不太强大的机器网络构建成一个庞大的系统。例如，如果您有100个节点，每个节点都有1TB的磁盘存储，那么您拥有100TB的HDFS空间。如果复制因子等于3，则可以存储大小为33TB的单个文件。</p><blockquote class="ld le lf"><p id="fb16" class="if ig lg ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated"><em class="hi">更别说有些本地文件系统不支持这么大的文件，就算你有可用的磁盘空间。</em></p></blockquote><h2 id="8b35" class="kp jn hi bd jo kq kr ks js kt ku kv jw iq kw kx ka iu ky kz ke iy la lb ki lc bi translated">阅读的速度</h2><p id="1eaa" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">如果你按顺序读这个文件，它将花费你<code class="du lk ll lm ln b">N</code>。但是如果文件被分割成10个节点之间的10个块，你可以在<code class="du lk ll lm ln b">N/10</code>时间内得到它的内容！因为每个节点都可以并行读取其数据块。所以，HDFS不仅仅关乎安全。这是关于迅捷。</p><blockquote class="ld le lf"><p id="ab5f" class="if ig lg ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated">我省略了花在网络交流上的时间。但是如果文件很大，这部分就是零头。</p></blockquote><h1 id="f892" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">Apache Hive的基础知识</h1><p id="2a2f" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">Apache Hive是运行在HDFS的数据库设施。它允许用HQL(类似SQL的语言)查询数据。</p><p id="d4ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">常规数据库(例如<a class="ae jd" rel="noopener" href="/javarevisited/7-best-free-postgresql-courses-for-beginners-to-learn-in-2021-3bf369d73794"> PostgreSQL </a>、<a class="ae jd" rel="noopener" href="/javarevisited/6-best-pl-sql-and-oracle-courses-for-beginners-to-learn-online-effd07d5fd2"> Oracle </a>)充当本地文件系统上的抽象层。而阿帕奇蜂巢则是HDFS上空的一个抽象。就是这样。</p><h1 id="332b" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">Apache Spark的基础知识</h1><p id="0d09" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated"><a class="ae jd" rel="noopener" href="/javarevisited/5-best-apache-spark-courses-for-java-and-python-developers-bbd9d63eb76c"> Apache Spark </a>是一个操作和转换海量数据的平台。关键思想是Apache Spark workers在多个节点上运行，并将中间结果存储在RAM中。它是用<a class="ae jd" rel="noopener" href="/javarevisited/7-best-scala-frameworks-for-concurrency-web-development-and-big-data-to-learn-fbd52dbe0a9a"> Scala </a>编写的，但是它也支持<a class="ae jd" rel="noopener" href="/javarevisited/10-best-java-development-tools-you-can-learn-66f7d4d837e6"> Java </a>和<a class="ae jd" rel="noopener" href="/javarevisited/top-10-tools-python-programmers-should-learn-cb9c2535f57"> Python </a>。看看下面的模式。这是Apache Spark批处理作业的常见表示。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><a href="https://javarevisited.blogspot.com/2021/11/top-5-courses-to-learn-apache-spark-in.html"><div class="er es lo"><img src="../Images/7dd26d8725f612d8342e9b171cd5b505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PN5y-VeoycIz3l6A.png"/></div></a><p class="lp lq et er es lr ls bd b be z dx translated">Apache Spark架构</p></figure><p id="e666" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://www.java67.com/2018/04/5-free-apache-spark-course-for-java-scala-python-developers.html" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>从<code class="du lk ll lm ln b">Data Producer</code>加载数据，对其进行一些操作，并将结果放入<code class="du lk ll lm ln b">Data Consumer</code>(在我们的例子中，Apache Hive是数据生产者，Aerospike是数据消费者)。Apache Spark应用程序是一个包含转换逻辑的常规<code class="du lk ll lm ln b">.jar</code>文件。看看下面的例子。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">字数统计示例</p></figure><p id="5150" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个简单的字数统计应用程序。首先，我们加载<code class="du lk ll lm ln b">raw_data.txt</code> HDFS文件的内容。然后我们用<code class="du lk ll lm ln b">" "</code>拆分每一行，给每一个单词分配<code class="du lk ll lm ln b">1</code>，用单词减少结果，总结出整数。然后将获得的配对保存到<code class="du lk ll lm ln b">word_count.txt</code>。</p><p id="249a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">流程类似于<a class="ae jd" href="https://www.java67.com/2014/04/java-8-stream-examples-and-tutorial.html" rel="noopener ugc nofollow" target="_blank"> Java Stream API </a>。不同之处在于，每个lambda表达式都在workers上执行。因此，Spark将代码传输到远程机器，执行计算，并返回获得的结果。如果我们拥有足够数量的工作线程，我们可以继续处理以TB甚至zettabytes计算的数据量。</p><blockquote class="ld le lf"><p id="eb10" class="if ig lg ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated"><em class="hi">Apache Spark</em><a class="ae jd" rel="noopener" href="/javarevisited/5-free-courses-to-learn-apache-spark-in-2020-bdff2d60c800"><em class="hi"/></a><em class="hi">将代码交付给数据的方法有一些缺点。我们到了发展阶段再讨论。</em></p></blockquote><p id="2fa7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一个重要的方面是<em class="lg">懒惰</em>。就像<a class="ae jd" rel="noopener" href="/javarevisited/7-best-java-tutorials-and-books-to-learn-lambda-expression-and-stream-api-and-other-features-3083e6038e14"> Stream API一样，</a> Apache Spark直到终端操作调用时才开始任何计算。在这种情况下，<code class="du lk ll lm ln b">reduceByKey</code>就是其中之一。rest操作构建管道规则，但不触发任何东西。</p><h1 id="5c5d" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">构建配置</h1><p id="09e7" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">让我们开始开发过程。首先，我们需要选择Java版本。在撰写本文时，最新稳定的Apache Spark版本是3.2.1。它支持Java 11。所以，我们要用它。</p><blockquote class="ld le lf"><p id="a197" class="if ig lg ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated"><em class="hi">目前Apache Spark不支持Java 17。确保不要用它来运行集成测试。否则，您会得到奇怪的错误消息。</em></p></blockquote><p id="a775" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该项目是用<a class="ae jd" href="https://start.spring.io/" rel="noopener ugc nofollow" target="_blank"> Spring Initializr </a>引导的。这里没什么特别的。但是应该澄清依赖列表。</p><h2 id="cd05" class="kp jn hi bd jo kq kr ks js kt ku kv jw iq kw kx ka iu ky kz ke iy la lb ki lc bi translated">依赖性解析</h2><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">build.gradle依赖项</p></figure><p id="12c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">核心依赖关系</strong></p><p id="9d4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先是Apache Spark依赖项。人工制品是根源。<code class="du lk ll lm ln b">spark-hive</code>支持从Apache Hive中检索数据。并且<code class="du lk ll lm ln b">spark-sql</code>依赖项使我们能够使用<a class="ae jd" rel="noopener" href="/javarevisited/8-best-resources-to-learn-sql-online-for-free-b00506d88c91"> SQL </a>从Apache Hive中查询数据。</p><blockquote class="ld le lf"><p id="48fc" class="if ig lg ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated"><em class="hi">注意，所有的工件必须共享同一个版本(在我们的例子中，它是</em> <code class="du lk ll lm ln b"><em class="hi">3.2.1</em></code> <em class="hi">)。事实上，Apache Spark依赖项的版本应该与您公司中运行生产集群的版本相匹配。</em></p></blockquote><p id="a0f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有火花依赖都必须标记为<code class="du lk ll lm ln b">compileOnly</code>。这意味着它们不会包含在汇编的<code class="du lk ll lm ln b">.jar</code>文件中。Apache Spark将在运行时提供所需的依赖关系。如果您将它们包含在<code class="du lk ll lm ln b">implementation</code>范围内，那可能会导致在执行过程中出现难以跟踪的错误。</p><p id="847a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后我们有了<code class="du lk ll lm ln b">aerospike-client</code>的依赖。您可能已经注意到<code class="du lk ll lm ln b">org.slf4j</code>组在任何地方都被排除在外，并且作为一个<code class="du lk ll lm ln b">compileOnly</code>依赖项包含在内。我们将在稍后到达Apache Spark日志记录设施时讨论这个问题。</p><p id="a333" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">测试依赖关系</strong></p><p id="4c60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，出现了测试范围的工件。阿帕奇Spark ones被收录为<code class="du lk ll lm ln b">testImplementation</code>。因为集成测试将启动本地Spark节点。因此，它们在运行时是必需的。<code class="du lk ll lm ln b">slf4j-api</code>也是运行时依赖项。<a class="ae jd" href="https://www.testcontainers.org/" rel="noopener ugc nofollow" target="_blank"> Testcontainers </a>将用于运行Aerospike实例。Apache Spark在作业执行期间需要<code class="du lk ll lm ln b">janino</code>。我们需要<a class="ae jd" href="https://db.apache.org/derby/" rel="noopener ugc nofollow" target="_blank"> Apache Derby </a>来调优Apache Hive，以便在本地运行。我们很快就会谈到这一点。</p><h2 id="b4a4" class="kp jn hi bd jo kq kr ks js kt ku kv jw iq kw kx ka iu ky kz ke iy la lb ki lc bi translated">日志记录配置</h2><p id="7392" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">Apache Spark将<code class="du lk ll lm ln b">log4j</code>与<code class="du lk ll lm ln b">slf4j</code>包装器一起应用。但是默认的Spring Boot记录器是<code class="du lk ll lm ln b">logback</code>。由于在<a class="ae jd" href="https://javarevisited.blogspot.com/2011/01/how-classpath-work-in-java.html" rel="noopener ugc nofollow" target="_blank">类路径</a>中存在多个日志记录工具，这种设置会导致Spring上下文初始化期间出现异常。最简单的解决方法是排除所有自动配置的<a class="ae jd" href="https://www.java67.com/2021/10/how-to-set-logging-level-in-spring-boot-.html" rel="noopener ugc nofollow" target="_blank"> Spring Boot测井特性</a>。这没什么大不了的。无论如何，Apache Spark在运行时提供了自己的<code class="du lk ll lm ln b">slf4j</code>实现。所以，我们只需要将这个依赖关系作为<code class="du lk ll lm ln b">compileOnly</code>包含进来。这就足够了。</p><p id="1b8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用<a class="ae jd" rel="noopener" href="/javarevisited/5-best-gradle-courses-and-books-to-learn-in-2021-93f49ce8ff8e">grade</a>从Spring Boot项目中排除<code class="du lk ll lm ln b">logback</code>很容易。看看下面的例子。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">从Spring Boot自动配置中排除回退</p></figure><p id="0bfb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">可能出现的</strong> <code class="du lk ll lm ln b"><strong class="ih hj">application.yml</strong></code> <strong class="ih hj">问题</strong></p><p id="1f25" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lk ll lm ln b">snakeyml</code>排除需要特别注意。Spring Boot使用这个库来解析来自<code class="du lk ll lm ln b">.yml</code>文件的属性(例如<code class="du lk ll lm ln b">application.yml</code>)。一些Apache Spark版本使用相同的库进行内部操作。</p><p id="d4db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">问题是<a class="ae jd" rel="noopener" href="/javarevisited/10-free-spring-boot-tutorials-and-courses-for-java-developers-53dfe084587e"> Spring Boot </a>和<a class="ae jd" href="https://javarevisited.blogspot.com/2017/12/top-5-courses-to-learn-big-data-and.html" rel="noopener ugc nofollow" target="_blank">阿帕奇Spark </a>要求的版本不同。如果你把它从Spring Boot依赖中排除，而依赖Apache Spark提供的那个，你将面临<code class="du lk ll lm ln b">NoSuchMethodError</code> (Spring Boot调用Apache Spark提供的版本中没有的方法)。所以，我建议坚持使用<code class="du lk ll lm ln b">.properties</code>格式，去掉Spring Boot YAML自动配置。那将帮助你避免不必要的困难。看看下面的代码示例。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">排除GsonAutoConfiguration</p></figure><h1 id="641a" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">胖罐子</h1><p id="3fec" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">结果<code class="du lk ll lm ln b">.jar</code>将要提交给Apache Spark集群(例如<code class="du lk ll lm ln b"><a class="ae jd" href="https://spark.apache.org/docs/latest/submitting-applications.html" rel="noopener ugc nofollow" target="_blank">spark-submit</a></code> <a class="ae jd" href="https://spark.apache.org/docs/latest/submitting-applications.html" rel="noopener ugc nofollow" target="_blank">命令</a>)。因此，它应该包含所有运行时工件。不幸的是，标准的Spring Boot打包并没有按照Apache Spark期望的方式放置依赖项。因此，我们将使用<a class="ae jd" href="https://github.com/johnrengelman/shadow" rel="noopener ugc nofollow" target="_blank"> shadow-jar Gradle插件</a>。看看下面的例子。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">ShadowJar插件配置</p></figure><p id="f18a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们可以运行所有的测试，并用<code class="du lk ll lm ln b">./gradlew test shadowJar</code>命令构建产品。</p><h1 id="696c" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">开始开发</h1><p id="710f" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">现在我们可以进入开发过程了。</p><h2 id="a430" class="kp jn hi bd jo kq kr ks js kt ku kv jw iq kw kx ka iu ky kz ke iy la lb ki lc bi translated">阿帕奇火花配置</h2><p id="ca21" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">我们需要声明<code class="du lk ll lm ln b"><a class="ae jd" href="https://spark.apache.org/docs/3.2.1/api/java/index.html?org/apache/spark/api/java/JavaSparkContext.html" rel="noopener ugc nofollow" target="_blank">JavaSparkContext</a></code>和<code class="du lk ll lm ln b"><a class="ae jd" href="https://spark.apache.org/docs/3.2.1/api/java/org/apache/spark/sql/SparkSession.html" rel="noopener ugc nofollow" target="_blank">SparkSession</a></code>。第一个是所有操作的核心Apache Spark。而<code class="du lk ll lm ln b">SparkSession</code>是<code class="du lk ll lm ln b">spark-sql</code>项目的一部分。它允许我们用<code class="du lk ll lm ln b">SQL</code>查询数据(这对Apache Hive来说相当方便)。看看下面的弹簧配置。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">Spring Boot火花配置</p></figure><p id="67ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lk ll lm ln b">SparkConf</code>定义Apache Spark作业的配置键。正如你已经注意到的，有两种豆子用于不同的<a class="ae jd" href="https://www.baeldung.com/spring-profiles" rel="noopener ugc nofollow" target="_blank">弹簧轮廓</a>。<code class="du lk ll lm ln b">LOCAL</code>用于集成测试，<code class="du lk ll lm ln b">PROD</code>应用于生产环境。<code class="du lk ll lm ln b">PROD</code>配置没有声明任何属性，因为它们通常在<code class="du lk ll lm ln b">spark-submit</code> shell脚本中作为命令行参数传递。</p><p id="5520" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相反，<code class="du lk ll lm ln b">LOCAL</code>概要文件定义了正确运行所需的一组默认属性。以下是最重要的几个。</p><ol class=""><li id="d839" class="lv lw hi ih b ii ij im in iq lx iu ly iy lz jc ma mb mc md bi translated"><code class="du lk ll lm ln b">setMaster("local")</code>告诉Apache Spark启动单个本地节点。</li><li id="3974" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><code class="du lk ll lm ln b">javax.jdo.option.ConnectionURL</code>和<code class="du lk ll lm ln b">javax.jdo.option.ConnectionDriverName</code>声明Apache Hive元存储的JDBC连接。这就是我们添加Apache Derby作为项目依赖项的原因</li><li id="8734" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><code class="du lk ll lm ln b">spark.sql.catalogImplementation</code>表示本地文件应以Apache Hive兼容格式存储</li><li id="45e6" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><code class="du lk ll lm ln b">spark.sql.warehouse.dir</code>是存储Apache Hive数据的目录。这里我们使用临时目录。</li></ol><p id="86c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lk ll lm ln b">JavaSparkContext</code>接受已定义的<code class="du lk ll lm ln b">SparkConf</code>作为构造函数参数。同时<code class="du lk ll lm ln b">SparkSession</code>包裹现有的<code class="du lk ll lm ln b">JavaSparkContext</code>。注意，应该手动启用Apache Hive支持(<code class="du lk ll lm ln b">enableHiveSupport</code>)。</p><h2 id="5c9f" class="kp jn hi bd jo kq kr ks js kt ku kv jw iq kw kx ka iu ky kz ke iy la lb ki lc bi translated">创建Apache配置单元表</h2><p id="e9a6" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">当我们向生产Apache Spark集群提交应用程序时，我们可能不需要创建任何Apache Hive表。这些表很可能已经被其他人创建了。我们的目标是选择行并将数据传输到另一个存储。但是当我们在本地(或者在CI环境中)运行集成测试时，默认情况下没有表。所以，我们需要以某种方式创造它们。</p><p id="70dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个项目中，我们使用一个表— <code class="du lk ll lm ln b">media.subscriber_info</code>。它由两列组成。MSISDN(电话号码)和一些用户ID。</p><p id="0578" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在每次测试运行之前，我们必须删除以前的数据并添加新的行，以确保验证规则的一致性。实现这一点最简单的方法是声明创建和删除表的脚本。我们将把它们保存在<code class="du lk ll lm ln b">resources</code>目录中。看看下面的结构。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><a href="https://javarevisited.blogspot.com/2018/02/top-5-restful-web-services-with-spring-courses-for-experienced-java-programmers.html"><div class="er es mj"><img src="../Images/a4d278a21aff8f0f08408488ac61634e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/0*JifOL8iKwB4NaBz0.png"/></div></a><p class="lp lq et er es lr ls bd b be z dx translated">Apache Hive DDL脚本</p></figure><p id="3204" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">V1 _媒体. hql </strong></p><p id="6998" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果没有的话，创建<code class="du lk ll lm ln b">media</code>数据库。</p><pre class="jf jg jh ji fd mk ln ml mm aw mn bi"><span id="18ed" class="kp jn hi ln b fi mo mp l mq mr">create database if not exists media</span></pre><p id="88a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">V2 _ _ media . subscriber _ info . hql</strong></p><p id="8d20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果不存在，则创建<code class="du lk ll lm ln b">subscriber_info</code>表。</p><pre class="jf jg jh ji fd mk ln ml mm aw mn bi"><span id="c1af" class="kp jn hi ln b fi mo mp l mq mr">create table if not exists media.subscriber_info (<br/>  subscriber_id string,<br/>  msisdn string<br/>)<br/>row format delimited<br/>fields terminated by ','<br/>lines terminated by '\n'<br/>stored as textfile</span></pre><p id="1e5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">掉V1 _ _ mediatv _ DDS . subscriber _ info . hql</strong></p><p id="73c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">掉落了<code class="du lk ll lm ln b">subscriber_info</code>的桌子。</p><pre class="jf jg jh ji fd mk ln ml mm aw mn bi"><span id="373e" class="kp jn hi ln b fi mo mp l mq mr">drop table if exists media.subscriber_info</span></pre><blockquote class="ld le lf"><p id="0bcb" class="if ig lg ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated"><code class="du lk ll lm ln b"><em class="hi">V[N]</em></code> <em class="hi">前缀不是必须的。我放置它们是为了确保每个新的表脚本都将作为最后一个执行。让测试确定性地工作是有帮助的。</em></p></blockquote><p id="933f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好了，现在我们需要一个处理程序来处理这些HQL查询。看看下面的例子。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">Apache Hive DDL脚本处理程序</p></figure><p id="b5fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先要注意的是<code class="du lk ll lm ln b">@Profile(LOCAL)</code>用法。因为我们不需要在生产环境中创建或删除表。</p><p id="9644" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lk ll lm ln b">createTables</code>和<code class="du lk ll lm ln b">dropTables</code>方法提供了包含所需查询的资源列表。</p><blockquote class="ld le lf"><p id="c455" class="if ig lg ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated"><code class="du lk ll lm ln b"><em class="hi">getResources</em></code> <em class="hi">是从</em> <a class="ae jd" href="http://www.java67.com/2012/08/what-is-path-and-classpath-in-java-difference.html" rel="noopener ugc nofollow" target="_blank"> <em class="hi">类路径</em> </a> <em class="hi">中读取文件的实用函数。你可以在这里</em>  <em class="hi">发现</em> <a class="ae jd" href="https://github.com/SimonHarmonicMinor/apache-spark-integration-testing-example/blob/master/src/main/java/com/mts/metric/spark/util/ResourceUtil.java" rel="noopener ugc nofollow" target="_blank"> <em class="hi">的实现。</em></a></p></blockquote><p id="cd77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，现在我们已经准备好编写业务代码了！</p><h1 id="8682" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">商业代码</h1><h2 id="0e19" class="kp jn hi bd jo kq kr ks js kt ku kv jw iq kw kx ka iu ky kz ke iy la lb ki lc bi translated">外表</h2><p id="c86e" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">核心接口是<code class="du lk ll lm ln b">EnricherService</code></p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">核心接口</p></figure><p id="f533" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们预计它可能有许多实现。每一个都代表整个批处理过程中一个步骤。</p><p id="704b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后我们有封装了所有<code class="du lk ll lm ln b">EnricherService</code>实现的<code class="du lk ll lm ln b">EnricherServiceFacade</code>，并逐个运行它们。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">EnricherServiceFacade</p></figure><p id="bc23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们正在尝试运行每一个提供的浓缩步骤。如果其中任何一个错误失败，我们抛出异常，将所有错误合并成一个整体。</p><p id="348a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们需要告诉Spring在应用程序启动时执行<code class="du lk ll lm ln b">EnricherServiceFacade.proceedEnrichment</code>。我们可以将它直接添加到<code class="du lk ll lm ln b">main</code>方法中，但它不是<em class="lg">弹簧方式</em>。因此，这增加了测试的难度。更好的选择是<code class="du lk ll lm ln b">@EventListener</code>。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">主监听器</p></figure><p id="5139" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当<a class="ae jd" href="https://javarevisited.blogspot.com/2020/05/top-20-spring-boot-interview-questions-answers.html" rel="noopener ugc nofollow" target="_blank"> Spring context </a>启动时，调用<code class="du lk ll lm ln b">proceedEnrichment</code>方法。顺便说一下，只有激活的<code class="du lk ll lm ln b">PROD</code>配置文件才会触发作业。</p><h2 id="6880" class="kp jn hi bd jo kq kr ks js kt ku kv jw iq kw kx ka iu ky kz ke iy la lb ki lc bi translated">EnricherService实现</h2><p id="4f42" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">我们将处理一个单独的<code class="du lk ll lm ln b">EnricherService</code>实现。它只是从<code class="du lk ll lm ln b">media.subcriber_info</code>表中选择所有行，并将结果放入Aerospike数据库。看看下面的代码片段。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">SubscriberIdEnricherService</p></figure><p id="8be0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有许多问题需要澄清。</p><p id="adfe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">序列化</strong></p><p id="8118" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Apache Spark应用了标准的Java序列化机制。因此，<a class="ae jd" rel="noopener" href="/javarevisited/8-best-lambdas-stream-and-functional-programming-courses-for-java-developers-3d1836a97a1d"> lambdas </a> ( <code class="du lk ll lm ln b">map</code>、<code class="du lk ll lm ln b">filter</code>、<code class="du lk ll lm ln b">groupBy</code>、<code class="du lk ll lm ln b">forEach</code>等)中使用的任何依赖项。)必须实现<code class="du lk ll lm ln b">Serializable</code>接口。否则，您将在运行时得到<code class="du lk ll lm ln b">NotSerializableException</code>。</p><p id="37d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在<code class="du lk ll lm ln b">foreachPartition</code>回调中引用了<code class="du lk ll lm ln b">AerospikeProperties</code>。因此，应该允许这个类和<code class="du lk ll lm ln b">SubscriberIdEnricherService</code>本身进行序列化(因为后者将<code class="du lk ll lm ln b">AerospikeProperties</code>保留为一个字段)。如果在任何Apache Spark lambda中没有使用依赖项，可以将其标记为<code class="du lk ll lm ln b">transient</code>。</p><p id="76fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，<code class="du lk ll lm ln b">serialVersionUID</code>手动分配至关重要。原因是Apache Spark可能会多次序列化和反序列化传递的对象。并且不能保证每次自动生成的<code class="du lk ll lm ln b">serialVersionUID</code>都是一样的。这可能是难以跟踪浮动错误的原因。为了防止这种情况，你应该自己申报<code class="du lk ll lm ln b">serialVersionUID</code>。</p><p id="3e93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更好的方法是强制编译器在任何<code class="du lk ll lm ln b">Serializable</code>类上验证<code class="du lk ll lm ln b">serialVersionUID</code>字段的存在。在这种情况下，您需要将<code class="du lk ll lm ln b">-Xlint:serial</code>警告标记为错误。看看Gradle的例子。</p><pre class="jf jg jh ji fd mk ln ml mm aw mn bi"><span id="99b7" class="kp jn hi ln b fi mo mp l mq mr">tasks.withType(JavaCompile) {<br/>    options.compilerArgs &lt;&lt; "-Xlint:serial" &lt;&lt; "-Werror" <br/>}</span></pre><p id="7b70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">气塞客户端实例化</strong></p><p id="78be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不幸的是，Java Aerospike客户端没有实现<code class="du lk ll lm ln b">Serializable</code>接口。所以，我们必须在lambda表达式中实例化它。在这种情况下，将直接在worker节点上创建对象。它使得序列化变得多余。</p><blockquote class="ld le lf"><p id="7f53" class="if ig lg ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated"><em class="hi">我应该承认Aerospike提供了</em><a class="ae jd" href="https://docs.aerospike.com/connect/spark/configuration" rel="noopener ugc nofollow" target="_blank"><em class="hi">Aerospike Connect Framework</em></a><em class="hi">允许通过Apache Spark以声明的方式传输数据，而无需创建任何Java客户端。反正你要用，就得直接把打包好的库安装到Apache Spark集群上。不能保证你在这种情况下会有这样的机会。所以，我省略了这个场景。</em></p></blockquote><p id="d181" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">分区</strong></p><p id="a51e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lk ll lm ln b">Dataset</code>类有<code class="du lk ll lm ln b">foreach</code>方法，该方法简单地为每个当前行执行给定的lambda。然而，如果你在回调中初始化一些繁重的资源(例如数据库连接)，新的资源将为每一行创建(在某些情况下可能有数十亿行)。不是很有效率吧？</p><p id="6748" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lk ll lm ln b">foreachPartition</code>方法的工作方式稍有不同。Apache Spark在每个<code class="du lk ll lm ln b">Dataset</code>分区执行一次。它也接受<code class="du lk ll lm ln b">Iterator&lt;Row&gt;</code>作为参数。因此，在lambda中，我们可以初始化“重”资源(例如<code class="du lk ll lm ln b">AerospikeClient</code>)并将它们应用于迭代器中每个<code class="du lk ll lm ln b">Row</code>的计算。</p><blockquote class="ld le lf"><p id="8bbf" class="if ig lg ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated"><em class="hi">分区大小是根据输入源和Apache Spark集群配置自动计算的。虽然您可以通过调用</em> <code class="du lk ll lm ln b"><a class="ae jd" href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html#repartition-org.apache.spark.sql.Column...-" rel="noopener ugc nofollow" target="_blank"><em class="hi">repartition</em></a></code> <em class="hi">方法来手动设置它。反正不在文章范围之内。</em></p></blockquote><h1 id="80f2" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">测试</h1><h2 id="8cb2" class="kp jn hi bd jo kq kr ks js kt ku kv jw iq kw kx ka iu ky kz ke iy la lb ki lc bi translated">气塞式安装</h2><p id="b7e9" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">好了，我们已经写了一些业务代码。我们如何测试它？首先，让我们为<a class="ae jd" href="https://www.testcontainers.org/" rel="noopener ugc nofollow" target="_blank">测试容器</a>声明Aerospike设置。看看下面的代码片段。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">集成套件</p></figure><p id="1118" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lk ll lm ln b">IntegrationSuite</code>类被用作所有集成测试的父类。<code class="du lk ll lm ln b">IntegrationSuite.Initializer</code>内部类被用作Spring上下文初始化器。当所有属性和bean定义都已经加载，但是还没有创建bean时，框架调用它。它允许我们在运行时覆盖一些属性。</p><p id="c52b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将Aerospike容器声明为<code class="du lk ll lm ln b">GenericContainer</code>，因为该库不提供对数据库的开箱即用支持。然后在<code class="du lk ll lm ln b">initialize</code>方法中，我们检索容器的主机和端口，并将它们分配给<code class="du lk ll lm ln b">aerospike.hosts</code>属性。</p><h2 id="06af" class="kp jn hi bd jo kq kr ks js kt ku kv jw iq kw kx ka iu ky kz ke iy la lb ki lc bi translated">Apache Hive实用程序</h2><p id="fba3" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">在每个测试方法之前，我们应该删除Apache Hive中的所有数据，并添加当前场景所需的新行。所以，测试不会互相影响。让我们为Apache Hive声明一个定制的测试外观。看看下面的代码片段。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">睾丸组织</p></figure><p id="57f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">只有两种方法。<code class="du lk ll lm ln b">cleanHive</code>删除所有现有的并重新创建。因此，所有以前的数据都将被擦除。这个<code class="du lk ll lm ln b">insertInto</code>很棘手。它的目的是以静态类型的方式向Apache Hive插入新行。这是怎么做到的？首先我们来考察一下<code class="du lk ll lm ln b">HiveTable&lt;T&gt;</code>接口。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="bace" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如你所见，这是一个普通的Java函数接口。尽管实现不是很明显。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">订户信息</p></figure><p id="c1fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该类接受<code class="du lk ll lm ln b">SparkSession</code>作为构造函数依赖。<code class="du lk ll lm ln b">SubscriberInfo.Values</code>是通用参数。类表示包含要插入的值的数据结构。最后，<code class="du lk ll lm ln b">values</code>实现执行实际的新行创建。</p><p id="79a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关键是<code class="du lk ll lm ln b">subscriberInfo</code>静态方法。退回<code class="du lk ll lm ln b">Function&lt;SparkSession, SubscriberInfo&gt;</code>的理由是什么？它与<code class="du lk ll lm ln b">TestHiveUtils.insertInto</code>的结合为我们提供了静态类型化的<code class="du lk ll lm ln b">INSERT INTO</code>语句。看看下面的代码示例。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">将数据插入Apache配置单元</p></figure><p id="0052" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个优雅的解决方案，你不觉得吗？</p><h2 id="be39" class="kp jn hi bd jo kq kr ks js kt ku kv jw iq kw kx ka iu ky kz ke iy la lb ki lc bi translated">火花积分测试片</h2><p id="5d23" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">Spring集成测试需要特定的配置。明智的做法是声明一次，重复使用。看看下面的代码片段。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">SparkIntegrationTestSuite</p></figure><p id="e054" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<code class="du lk ll lm ln b"><a class="ae jd" href="https://javarevisited.blogspot.com/2021/12/what-is-springboottest-annotation-in.html" rel="noopener ugc nofollow" target="_blank">SpringBootTest</a></code> <a class="ae jd" href="https://javarevisited.blogspot.com/2021/12/what-is-springboottest-annotation-in.html" rel="noopener ugc nofollow" target="_blank"> </a>中，我们列出了测试运行期间使用的所有beans。</p><blockquote class="ld le lf"><p id="84c7" class="if ig lg ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated"><code class="du lk ll lm ln b"><em class="hi">TestAerospikeFacade</em></code> <em class="hi">只是Java Aerospike客户端的一个薄薄的包装，用于测试目的。它的实现相当简单，但是你可以通过</em> <a class="ae jd" href="https://github.com/SimonHarmonicMinor/apache-spark-integration-testing-example/blob/master/src/test/java/com/mts/metric/spark/testutils/facade/TestAerospikeFacade.java" rel="noopener ugc nofollow" target="_blank"> <em class="hi">这个链接</em> </a> <em class="hi">查看源代码。</em></p></blockquote><p id="7a5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lk ll lm ln b">EnricherServiceTestConfiguration</code>是Spring配置，声明了<code class="du lk ll lm ln b">EnricherService</code>接口的所有实现。看看下面的例子。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">EnricherService测试配置</p></figure><blockquote class="ld le lf"><p id="9778" class="if ig lg ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated"><em class="hi">我想指出的是</em> <strong class="ih hj"> <em class="hi">所有的</em> </strong> <em class="hi"> </em> <code class="du lk ll lm ln b"><em class="hi">EnricherService</em></code> <em class="hi">实现都应该列在类里面。如果我们为每个测试套件应用不同的配置，Spring上下文将被重新加载。大多数情况下，这不是问题。但是Apache Spark的使用带来了障碍。你看，当</em> <code class="du lk ll lm ln b"><em class="hi">JavaSparkContext</em></code> <em class="hi">被创建时，它启动了本地的Apache Spark节点。但是当我们在应用程序生命周期中实例化它两次时，就会导致一个异常。解决这个问题最简单的方法是确保</em> <code class="du lk ll lm ln b"><em class="hi">JavaSparkContext</em></code> <em class="hi">只被创建一次。</em></p></blockquote><p id="92a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们可以进入测试过程了。</p><h2 id="df8d" class="kp jn hi bd jo kq kr ks js kt ku kv jw iq kw kx ka iu ky kz ke iy la lb ki lc bi translated">集成测试示例</h2><p id="9568" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">下面是一个简单的集成测试，它向Apache Spark插入两行，并检查相应的两个记录是否在10秒钟内在Aerospike中创建。请看下面的代码片段。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lt lu l"/></div><p class="lp lq et er es lr ls bd b be z dx translated">subscriberidernicherserviceintegrationtest</p></figure><p id="c6df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果一切都调整正确，测试将会通过。</p><blockquote class="ld le lf"><p id="b4d4" class="if ig lg ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated"><em class="hi">整个测试源可通过</em><a class="ae jd" href="https://github.com/SimonHarmonicMinor/apache-spark-integration-testing-example/blob/master/src/test/java/com/mts/metric/spark/service/SubscriberIdEnricherServiceIntegrationTest.java" rel="noopener ugc nofollow" target="_blank"><em class="hi"/></a><em class="hi">这个环节获得。</em></p></blockquote><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="er es ms"><img src="../Images/0c0ffe2f282a4b0fc701031993053436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8rAmLAfMMdOjK6yp.png"/></div></div></figure><h1 id="44ea" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">结论</h1><p id="65a1" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">这就是我想告诉你的关于测试Apache Hive、Apache Spark和Aerospike与Spring Boot用法的集成的全部内容。如你所见，大数据世界终究没有那么复杂。所有代码示例都取自<a class="ae jd" href="https://github.com/SimonHarmonicMinor/apache-spark-integration-testing-example" rel="noopener ugc nofollow" target="_blank">这个库</a>。您可以克隆它并自己进行测试。</p><p id="9ab4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您有任何问题或建议，请在下面留下您的评论。感谢阅读！</p><h1 id="3b46" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">资源</h1><ol class=""><li id="5c0b" class="lv lw hi ih b ii kk im kl iq mx iu my iy mz jc ma mb mc md bi translated"><a class="ae jd" href="https://github.com/SimonHarmonicMinor/apache-spark-integration-testing-example" rel="noopener ugc nofollow" target="_blank">带有示例的存储库</a></li><li id="637c" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><a class="ae jd" href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html" rel="noopener ugc nofollow" target="_blank"> HDFS (Hadoop分布式文件系统)</a></li><li id="4947" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><a class="ae jd" href="https://hive.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇蜂巢</a></li><li id="d02d" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><a class="ae jd" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇火花</a></li><li id="c628" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><a class="ae jd" href="https://db.apache.org/derby/" rel="noopener ugc nofollow" target="_blank">阿帕奇德比</a></li><li id="bcfc" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><a class="ae jd" href="https://aerospike.com/" rel="noopener ugc nofollow" target="_blank">气塞式气塞数据库</a></li><li id="72e8" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><a class="ae jd" href="https://docs.aerospike.com/connect/spark/configuration" rel="noopener ugc nofollow" target="_blank">气塞连接框架</a></li><li id="d8ec" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><a class="ae jd" href="https://www.baeldung.com/java-8-streams" rel="noopener ugc nofollow" target="_blank"> Java流API </a></li><li id="0982" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><a class="ae jd" href="https://start.spring.io/" rel="noopener ugc nofollow" target="_blank">弹簧初始状态</a></li><li id="a8db" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><a class="ae jd" href="https://www.baeldung.com/spring-profiles" rel="noopener ugc nofollow" target="_blank">弹簧型材</a></li><li id="c919" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><a class="ae jd" href="https://www.testcontainers.org/" rel="noopener ugc nofollow" target="_blank">测试容器</a></li><li id="8984" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated"><a class="ae jd" href="https://github.com/johnrengelman/shadow" rel="noopener ugc nofollow" target="_blank"> Gradle插件shadow-jar </a></li></ol></div></div>    
</body>
</html>